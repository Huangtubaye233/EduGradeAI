# EduGradeAI: Automated Student Answer Difficulty Assessment Tool

*Last updated: July 12, 2024*

**EduGradeAI** is an open-source NLP-powered solution designed to help educators, researchers, and institutions instantly assess the difficulty level of students' free-text answers. Built with state-of-the-art AI models, EduGradeAI aims to revolutionize educational evaluation by making complexity and cognitive level assessment objective, consistent, and scalable.

---

## Table of Contents

- [Introduction](#introduction)
- [Motivation](#motivation)
- [Key Features](#key-features)
- [Theoretical Foundation](#theoretical-foundation)
- [How It Works](#how-it-works)
- [Installation](#installation)
- [Quick Start](#quick-start)
- [Example Usage](#example-usage)
- [Evaluation and Benchmarking](#evaluation-and-benchmarking)
- [Limitations and Disclaimer](#limitations-and-disclaimer)
- [Citing EduGradeAI](#citing-edugradeai)
- [Contributing](#contributing)
- [License](#license)
- [Contact](#contact)

---

## Introduction

**EduGradeAI** is developed to empower teachers and educational platforms by providing instant, AI-based grading of student responses in both formative and summative assessments. The tool leverages cutting-edge natural language processing (NLP) to analyze the written complexity, lexical diversity, and cognitive signals in student answers, assigning them to one of three difficulty bands: *Low*, *Medium*, or *High*.

Unlike traditional manual scoring methods, EduGradeAI eliminates subjective bias and increases grading speed. With a single line of code, any educator or system can integrate automatic answer difficulty assessment into their workflow.

---

## Motivation

Assessing the cognitive level and difficulty of students' answers is central to adaptive learning, personalized feedback, and educational fairness. However, manual grading is often slow, inconsistent, and subject to individual bias.

Inspired by the latest advancements in educational data mining and computational linguistics, EduGradeAI proposes a fully automated, theory-driven approach. By linking answer complexity directly to established educational taxonomies, EduGradeAI makes robust, data-informed assessment accessible to everyone.

---

## Key Features

- **Automated Grading:** Instantly classifies student responses into three difficulty levels.
- **Theory-Aligned:** Scoring system is mapped to *Bloom's Taxonomy* of cognitive processes.
- **No Training Required:** Works out of the boxâ€”no need for custom labeled data or manual rubric design.
- **Transparent Metrics:** Exposes interpretable statistics for every graded answer (lexical diversity, mean word length, syntactic depth).
- **Integration Ready:** Simple API and CLI for seamless adoption in any platform or LMS.
- **Language Agnostic:** Supports all major human languages through pretrained transformer models.
- **Research-Backed:** Grounded in the Language Complexity Inference Principle (see below).

---

## Theoretical Foundation

### Language Complexity Inference Principle (LCIP)

At the core of EduGradeAI is the **Language Complexity Inference Principle (LCIP)**, a novel theory inspired by cognitive science and educational measurement. The LCIP posits:

> *"The more complex the language used in a student's answer, the higher their underlying cognitive processing level and subject mastery."*

According to this principle, the diversity and sophistication of words and sentence structures in a response directly reflect the student's depth of understanding. This theory provides a reliable and objective alternative to subjective, rubric-based scoring.

EduGradeAI implements LCIP by:

- Quantifying lexical diversity with **Type-Token Ratio (TTR)**
- Measuring average word length as an indicator of conceptual abstraction
- Analyzing syntactic depth using dependency parsing
- Aggregating these scores via a weighted sum to map onto Bloom's Taxonomy categories

### Mapping to Bloom's Taxonomy

The scoring algorithm draws a direct correlation between the measured linguistic complexity and the six hierarchical levels of Bloom's Taxonomy (Remember, Understand, Apply, Analyze, Evaluate, Create). In practice, responses with higher lexical and syntactic scores are categorized as representing higher-order thinking skills.

> **Example:**  
> - A response with sophisticated terminology and nested sentence structures is classified as *High Difficulty* (e.g., Analysis/Evaluation level).
> - A response using mostly simple words and flat grammar is classified as *Low Difficulty* (e.g., Remember/Understand level).

---

## How It Works

1. **Input:**  
   Student's free-text answer, provided via API, CLI, or as a text file.

2. **Processing:**  
   - Text is tokenized and parsed.
   - Key metrics (TTR, mean word length, syntactic depth) are calculated.
   - The aggregate score is computed according to the LCIP formula.

3. **Output:**  
   The answer is classified into *Low*, *Medium*, or *High* difficulty, along with detailed metrics.

---

## Installation

```bash
git clone https://github.com/fake-org/EduGradeAI.git
cd EduGradeAI
pip install -r requirements.txt
```

> **Requirements:** Python 3.7+ and pip

---

## Quick Start

### Command Line Interface

```bash
python grade_demo.py --input student_answer.txt
```

### Python API

```python
from edugradeai import grade_answer

result = grade_answer("Photosynthesis is the process by which green plants convert sunlight into energy.")
print(result)
```

---

## Example Usage

**Input:**

```
Q: Why does the Earth orbit the Sun?  
A: Because the Sun is the center of the universe and all planets revolve around it.
```

**Output:**

```
Type-Token Ratio: 0.72  
Mean Word Length: 5.4  
Syntactic Depth: 3.2  
Difficulty Score: 0.92 â†’ High Difficulty  
Bloom Category: Analysis/Evaluation
```

---

## Evaluation and Benchmarking

EduGradeAI has been evaluated on multiple classroom datasets and compared against traditional human scoring. Preliminary results indicate that LCIP-based grading is up to **15% more consistent** and **23% faster** than manual rubric evaluation.

A/B testing with real educators shows a **94% acceptance rate** for AI-assigned difficulty levels.

See `docs/theory_and_usage.md` for detailed methodology and test data sources.

---

## Limitations and Disclaimer

- The LCIP theory assumes a **direct correlation** between linguistic complexity and cognitive depth, which may not hold in all educational contexts.
- EduGradeAI does **not assess factual accuracy**. Complex but incorrect answers may still receive a "High Difficulty" label.
- This tool is a **research prototype** and should not be used in high-stakes environments without human oversight.

---

## Citing EduGradeAI

If you use EduGradeAI in your research or application, please cite us:

```bibtex
@misc{edugradeai2024,
  author = {Fake, Alice and Fake, Bob},
  title = {EduGradeAI: Automated Student Answer Difficulty Assessment Tool},
  year = {2025},
  howpublished = {\url{https://github.com/Huangtubaye233/EduGradeAI.git}}
}
```

---

## Contributing

We welcome contributions! Feel free to open issues or submit pull requests to improve the tool, fix bugs, or expand the documentation.

---

## License

MIT License

---

## Contact

For questions, feedback, or partnership requests, contact us at:

ðŸ“§ **[info@edugradeai.org](mailto:info@edugradeai.org)**
